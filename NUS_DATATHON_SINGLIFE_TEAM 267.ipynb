{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas \n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install imblearn\n",
    "%pip install pyarrow\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###...code...###\n",
    "# pd.show_versions()\n",
    "df = pd.read_parquet(filepath)\n",
    "df.head() # can only use 710 values to \n",
    "df['f_purchase_lh'] = df['f_purchase_lh'].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16978, 141)\n"
     ]
    }
   ],
   "source": [
    "# drop low variance columns\n",
    "columns = [col for col in df.columns if len(df[col].unique()) > 1]\n",
    "df = df[columns]\n",
    "\n",
    "\n",
    "# drop rows where the flag is NaN\n",
    "df = df[df['flg_substandard'].notna()]\n",
    "# drop some unwanted columns\n",
    "df = df.drop([\"clntnum\",\"ctrycode_desc\",\"flg_is_returned_mail\",\n",
    "              \"is_consent_to_mail\", \"is_consent_to_email\", \"is_consent_to_call\", \"is_consent_to_sms\",\"min_occ_date\",\"cltdob_fix\"], axis=1)\n",
    "unwanted_pattern = ['ape_', 'sumins_', 'prempaid_']\n",
    "unwanted_cols = [col for col in df.columns if any(pattern in col for pattern in unwanted_pattern)]\n",
    "df = df.drop(unwanted_cols, axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "columns = [col for col in df.columns if len(df[col].unique()) > 1]\n",
    "df = df[columns]\n",
    "# included_ape = [g for g in distinct_ape if g in columns]\n",
    "# included_sumins = [g for g in distinct_sumins if g in columns]\n",
    "# included_prempaid = [g for g in distinct_prempaid if g in columns]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16978, 140)\n",
      "(16978, 107)\n",
      "16278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 501, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 969, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1438, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute NaN values only for numerical columns using KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(df.shape)\n",
    "df = df.dropna(axis=1)\n",
    "print(df.shape)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Perform one-hot encoding on categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_category = pd.DataFrame(encoder.fit_transform(df[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate the encoded category with the original DataFrame\n",
    "df_encoded = pd.concat([df.drop(categorical_cols, axis=1), encoded_category], axis=1)\n",
    "df_encoded = df_encoded[df_encoded['f_purchase_lh'].notna()]\n",
    "# print(df_encoded)\n",
    "\n",
    "# Separate majority (label=0) and minority (label=1) classes\n",
    "majority_class = df_encoded[df_encoded['f_purchase_lh'] == 0]\n",
    "minority_class = df_encoded[df_encoded['f_purchase_lh'] == 1]\n",
    "print(len(majority_class))\n",
    "\n",
    "\n",
    "# minority_class_X = minority_class.drop('f_purchase_lh', axis=1).dropna(axis=1)\n",
    "# majority_class_X = majority_class.drop('f_purchase_lh', axis=1).dropna(axis=1)\n",
    "X = df_encoded.drop('f_purchase_lh', axis=1).dropna(axis=1)\n",
    "y = df_encoded['f_purchase_lh']\n",
    "\n",
    "\n",
    "# Downsample majority class FIRST\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('under', RandomUnderSampler(sampling_strategy=0.5)), ('over', SMOTE(sampling_strategy=0.75, random_state=42))])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "# counter_0 = 0\n",
    "# counter_1 = 0\n",
    "# for i in y_resampled:\n",
    "#     if i == 0:\n",
    "#         counter_0+=1\n",
    "#     else: \n",
    "#         counter_1 +=1\n",
    "# print(\"counter0\", counter_0)\n",
    "# print(\"counter1\", counter_1)\n",
    "\n",
    "# Upsample minority class using SMOTE\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "# minority_class_upsampled, _ = smote.fit_resample(minority_class_X, minority_class['f_purchase_lh'])\n",
    "\n",
    "# Downsample majority class\n",
    "# majority_class_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class_upsampled), random_state=42)\n",
    "\n",
    "# Combine the upsampled minority class with the downsampled majority class\n",
    "# balanced_df = pd.concat([majority_class_downsampled, minority_class_upsampled])\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "# balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "# X = balanced_df.drop('f_purchase_lh', axis=1)\n",
    "# y = balanced_df['f_purchase_lh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret classification and init setup\n",
    "from pycaret.classification import *\n",
    "s = setup(df, target = 'f_purchase_lh', session_id = 123,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = create_model('lr', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    result = predict_model(lr, data = hidden_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
